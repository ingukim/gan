{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "total_epoch=300\n",
    "batch_size=100\n",
    "n_hidden=256\n",
    "n_input=28*28\n",
    "n_noise=128\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None, n_input])\n",
    "Z=tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "G_W1=tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))\n",
    "G_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2=tf.Variable(tf.random_normal([n_hidden,n_input],stddev=0.01))\n",
    "G_b2=tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "D_W1=tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))\n",
    "D_b1=tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2=tf.Variable(tf.random_normal([n_hidden,1],stddev=0.01))\n",
    "D_b2=tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_z): #define generator\n",
    "    hidden=tf.nn.relu(tf.matmul(noise_z, G_W1)+G_b1)\n",
    "    output=tf.nn.sigmoid(tf.matmul(hidden,G_W2)+G_b2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs): #define discriminator\n",
    "    hidden=tf.nn.relu(tf.matmul(inputs, D_W1)+D_b1)\n",
    "    output=tf.nn.sigmoid(tf.matmul(hidden,D_W2)+D_b2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size, n_noise): #input generator\n",
    "    return np.random.normal(size=(batch_size, n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=generator(Z) #fake data 생성 \n",
    "D_gene=discriminator(G) #fake data를 판별한값\n",
    "D_real=discriminator(X) #real data를 판별한값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D=tf.reduce_mean(tf.log(D_real)+tf.log(1-D_gene)) #D_real을 1로 D_gene를 0으로 만들기위한 구조\n",
    "loss_G=tf.reduce_mean(tf.log(D_gene)) #D_gene를 1로 만들기 위한 loss  더 커지게 (음수로 커짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_var_list=[D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list=[G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D=tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
    "train_G=tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)\n",
    "#각 신경망 입장에서 loss가 커야 더 잘 학습된 상태를 나타냄.\n",
    "#때문에 Optimizer의 minimize(최소화) 함수에 전달할때 마이너스 부호를 붙힘.\n",
    "#var_list는 생성기 학습할때는 생성기를 구성하는 파라미터만, 판별기를 학습할때는 판별기를 구성하는 파라미터만 조정하도록함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: -0.8633 G loss: -1.518\n",
      "Epoch: 0001 D loss: -0.706 G loss: -1.612\n",
      "Epoch: 0002 D loss: -0.5782 G loss: -1.638\n",
      "Epoch: 0003 D loss: -0.3913 G loss: -1.769\n",
      "Epoch: 0004 D loss: -0.6197 G loss: -1.414\n",
      "Epoch: 0005 D loss: -0.4195 G loss: -1.634\n",
      "Epoch: 0006 D loss: -0.6205 G loss: -1.34\n",
      "Epoch: 0007 D loss: -0.5107 G loss: -1.663\n",
      "Epoch: 0008 D loss: -0.4732 G loss: -1.94\n",
      "Epoch: 0009 D loss: -0.379 G loss: -2.167\n",
      "Epoch: 0010 D loss: -0.2275 G loss: -2.562\n",
      "Epoch: 0011 D loss: -0.2011 G loss: -2.802\n",
      "Epoch: 0012 D loss: -0.5506 G loss: -1.983\n",
      "Epoch: 0013 D loss: -0.3292 G loss: -2.103\n",
      "Epoch: 0014 D loss: -0.4515 G loss: -2.111\n",
      "Epoch: 0015 D loss: -0.4451 G loss: -2.01\n",
      "Epoch: 0016 D loss: -0.4265 G loss: -2.248\n",
      "Epoch: 0017 D loss: -0.6348 G loss: -1.771\n",
      "Epoch: 0018 D loss: -0.4357 G loss: -2.11\n",
      "Epoch: 0019 D loss: -0.4358 G loss: -2.186\n",
      "Epoch: 0020 D loss: -0.5277 G loss: -1.947\n",
      "Epoch: 0021 D loss: -0.7216 G loss: -1.619\n",
      "Epoch: 0022 D loss: -0.5653 G loss: -1.951\n",
      "Epoch: 0023 D loss: -0.4146 G loss: -2.116\n",
      "Epoch: 0024 D loss: -0.3706 G loss: -2.433\n",
      "Epoch: 0025 D loss: -0.5068 G loss: -1.953\n",
      "Epoch: 0026 D loss: -0.5734 G loss: -1.859\n",
      "Epoch: 0027 D loss: -0.4342 G loss: -1.989\n",
      "Epoch: 0028 D loss: -0.393 G loss: -2.314\n",
      "Epoch: 0029 D loss: -0.3869 G loss: -2.394\n",
      "Epoch: 0030 D loss: -0.315 G loss: -2.354\n",
      "Epoch: 0031 D loss: -0.3546 G loss: -2.373\n",
      "Epoch: 0032 D loss: -0.3458 G loss: -2.367\n",
      "Epoch: 0033 D loss: -0.3913 G loss: -2.438\n",
      "Epoch: 0034 D loss: -0.2543 G loss: -2.478\n",
      "Epoch: 0035 D loss: -0.3112 G loss: -2.669\n",
      "Epoch: 0036 D loss: -0.6061 G loss: -2.187\n",
      "Epoch: 0037 D loss: -0.2658 G loss: -2.755\n",
      "Epoch: 0038 D loss: -0.3834 G loss: -2.627\n",
      "Epoch: 0039 D loss: -0.3082 G loss: -2.898\n",
      "Epoch: 0040 D loss: -0.2433 G loss: -2.847\n",
      "Epoch: 0041 D loss: -0.3116 G loss: -2.76\n",
      "Epoch: 0042 D loss: -0.2224 G loss: -3.103\n",
      "Epoch: 0043 D loss: -0.295 G loss: -2.652\n",
      "Epoch: 0044 D loss: -0.3448 G loss: -2.739\n",
      "Epoch: 0045 D loss: -0.3326 G loss: -2.715\n",
      "Epoch: 0046 D loss: -0.4982 G loss: -2.282\n",
      "Epoch: 0047 D loss: -0.695 G loss: -1.826\n",
      "Epoch: 0048 D loss: -0.1783 G loss: -3.234\n",
      "Epoch: 0049 D loss: -0.3393 G loss: -2.721\n",
      "Epoch: 0050 D loss: -0.3869 G loss: -2.49\n",
      "Epoch: 0051 D loss: -0.3948 G loss: -2.564\n",
      "Epoch: 0052 D loss: -0.2311 G loss: -3.5\n",
      "Epoch: 0053 D loss: -0.3898 G loss: -2.917\n",
      "Epoch: 0054 D loss: -0.2668 G loss: -3.165\n",
      "Epoch: 0055 D loss: -0.5167 G loss: -2.321\n",
      "Epoch: 0056 D loss: -0.1942 G loss: -3.235\n",
      "Epoch: 0057 D loss: -0.1907 G loss: -3.787\n",
      "Epoch: 0058 D loss: -0.4107 G loss: -2.648\n",
      "Epoch: 0059 D loss: -0.2696 G loss: -3.08\n",
      "Epoch: 0060 D loss: -0.2946 G loss: -2.862\n",
      "Epoch: 0061 D loss: -0.3916 G loss: -2.691\n",
      "Epoch: 0062 D loss: -0.2884 G loss: -3.132\n",
      "Epoch: 0063 D loss: -0.381 G loss: -2.945\n",
      "Epoch: 0064 D loss: -0.4072 G loss: -2.623\n",
      "Epoch: 0065 D loss: -0.403 G loss: -2.762\n",
      "Epoch: 0066 D loss: -0.1961 G loss: -3.212\n",
      "Epoch: 0067 D loss: -0.1587 G loss: -3.623\n",
      "Epoch: 0068 D loss: -0.3016 G loss: -2.865\n",
      "Epoch: 0069 D loss: -0.4862 G loss: -2.789\n",
      "Epoch: 0070 D loss: -0.3795 G loss: -3.017\n",
      "Epoch: 0071 D loss: -0.282 G loss: -2.903\n",
      "Epoch: 0072 D loss: -0.3951 G loss: -2.663\n",
      "Epoch: 0073 D loss: -0.306 G loss: -2.859\n",
      "Epoch: 0074 D loss: -0.3878 G loss: -2.898\n",
      "Epoch: 0075 D loss: -0.334 G loss: -3.185\n",
      "Epoch: 0076 D loss: -0.4103 G loss: -2.532\n",
      "Epoch: 0077 D loss: -0.1829 G loss: -3.527\n",
      "Epoch: 0078 D loss: -0.4002 G loss: -2.772\n",
      "Epoch: 0079 D loss: -0.3387 G loss: -2.723\n",
      "Epoch: 0080 D loss: -0.4032 G loss: -2.951\n",
      "Epoch: 0081 D loss: -1.869 G loss: -1.429\n",
      "Epoch: 0082 D loss: -0.2013 G loss: -3.696\n",
      "Epoch: 0083 D loss: -0.5359 G loss: -2.58\n",
      "Epoch: 0084 D loss: -0.3587 G loss: -2.697\n",
      "Epoch: 0085 D loss: -0.6019 G loss: -2.255\n",
      "Epoch: 0086 D loss: -0.2739 G loss: -2.819\n",
      "Epoch: 0087 D loss: -0.478 G loss: -2.221\n",
      "Epoch: 0088 D loss: -0.3458 G loss: -2.79\n",
      "Epoch: 0089 D loss: -0.3088 G loss: -3.056\n",
      "Epoch: 0090 D loss: -0.4153 G loss: -2.777\n",
      "Epoch: 0091 D loss: -0.3458 G loss: -3.034\n",
      "Epoch: 0092 D loss: -0.2028 G loss: -3.3\n",
      "Epoch: 0093 D loss: -0.2931 G loss: -3.183\n",
      "Epoch: 0094 D loss: -0.2275 G loss: -3.154\n",
      "Epoch: 0095 D loss: -0.4927 G loss: -2.628\n",
      "Epoch: 0096 D loss: -0.2418 G loss: -3.018\n",
      "Epoch: 0097 D loss: -0.3118 G loss: -2.887\n",
      "Epoch: 0098 D loss: -0.4709 G loss: -2.623\n",
      "Epoch: 0099 D loss: -0.3503 G loss: -2.979\n",
      "Epoch: 0100 D loss: -0.4113 G loss: -2.78\n",
      "Epoch: 0101 D loss: -0.3513 G loss: -2.896\n",
      "Epoch: 0102 D loss: -0.2155 G loss: -3.208\n",
      "Epoch: 0103 D loss: -0.4721 G loss: -2.947\n",
      "Epoch: 0104 D loss: -0.4164 G loss: -2.548\n",
      "Epoch: 0105 D loss: -0.9562 G loss: -2.052\n",
      "Epoch: 0106 D loss: -0.3488 G loss: -2.822\n",
      "Epoch: 0107 D loss: -0.3548 G loss: -2.957\n",
      "Epoch: 0108 D loss: -0.3194 G loss: -2.9\n",
      "Epoch: 0109 D loss: -0.3022 G loss: -2.871\n",
      "Epoch: 0110 D loss: -0.5106 G loss: -2.539\n",
      "Epoch: 0111 D loss: -0.5078 G loss: -2.756\n",
      "Epoch: 0112 D loss: -0.4561 G loss: -2.671\n",
      "Epoch: 0113 D loss: -0.4698 G loss: -2.591\n",
      "Epoch: 0114 D loss: -0.3522 G loss: -2.458\n",
      "Epoch: 0115 D loss: -0.2732 G loss: -2.766\n",
      "Epoch: 0116 D loss: -0.5545 G loss: -2.629\n",
      "Epoch: 0117 D loss: -0.3996 G loss: -2.612\n",
      "Epoch: 0118 D loss: -0.3917 G loss: -2.544\n",
      "Epoch: 0119 D loss: -0.8111 G loss: -1.954\n",
      "Epoch: 0120 D loss: -0.521 G loss: -2.599\n",
      "Epoch: 0121 D loss: -0.5154 G loss: -2.516\n",
      "Epoch: 0122 D loss: -0.3496 G loss: -2.543\n",
      "Epoch: 0123 D loss: -0.3187 G loss: -2.91\n",
      "Epoch: 0124 D loss: -0.3673 G loss: -2.805\n",
      "Epoch: 0125 D loss: -0.526 G loss: -2.257\n",
      "Epoch: 0126 D loss: -0.5064 G loss: -2.489\n",
      "Epoch: 0127 D loss: -0.5238 G loss: -2.379\n",
      "Epoch: 0128 D loss: -0.4456 G loss: -2.287\n",
      "Epoch: 0129 D loss: -0.2962 G loss: -3.037\n",
      "Epoch: 0130 D loss: -0.3757 G loss: -2.602\n",
      "Epoch: 0131 D loss: -0.3724 G loss: -2.624\n",
      "Epoch: 0132 D loss: -0.4967 G loss: -2.564\n",
      "Epoch: 0133 D loss: -0.5661 G loss: -2.575\n",
      "Epoch: 0134 D loss: -0.5153 G loss: -2.456\n",
      "Epoch: 0135 D loss: -0.4299 G loss: -2.91\n",
      "Epoch: 0136 D loss: -0.8493 G loss: -2.044\n",
      "Epoch: 0137 D loss: -0.7625 G loss: -2.177\n",
      "Epoch: 0138 D loss: -0.5811 G loss: -2.314\n",
      "Epoch: 0139 D loss: -0.4823 G loss: -2.773\n",
      "Epoch: 0140 D loss: -0.4583 G loss: -2.494\n",
      "Epoch: 0141 D loss: -0.423 G loss: -2.317\n",
      "Epoch: 0142 D loss: -0.3481 G loss: -2.817\n",
      "Epoch: 0143 D loss: -0.5215 G loss: -2.474\n",
      "Epoch: 0144 D loss: -0.522 G loss: -2.592\n",
      "Epoch: 0145 D loss: -0.3753 G loss: -2.768\n",
      "Epoch: 0146 D loss: -0.4388 G loss: -2.46\n",
      "Epoch: 0147 D loss: -0.5503 G loss: -2.958\n",
      "Epoch: 0148 D loss: -0.5344 G loss: -2.512\n",
      "Epoch: 0149 D loss: -0.4203 G loss: -3.005\n",
      "Epoch: 0150 D loss: -0.3862 G loss: -2.555\n",
      "Epoch: 0151 D loss: -0.5377 G loss: -2.662\n",
      "Epoch: 0152 D loss: -0.4191 G loss: -2.681\n",
      "Epoch: 0153 D loss: -0.4968 G loss: -2.446\n",
      "Epoch: 0154 D loss: -0.4385 G loss: -2.611\n",
      "Epoch: 0155 D loss: -0.4832 G loss: -2.519\n",
      "Epoch: 0156 D loss: -0.8449 G loss: -2.083\n",
      "Epoch: 0157 D loss: -0.3303 G loss: -2.58\n",
      "Epoch: 0158 D loss: -0.531 G loss: -2.549\n",
      "Epoch: 0159 D loss: -0.4346 G loss: -2.88\n",
      "Epoch: 0160 D loss: -0.4203 G loss: -2.456\n",
      "Epoch: 0161 D loss: -0.5481 G loss: -2.617\n",
      "Epoch: 0162 D loss: -0.4309 G loss: -2.722\n",
      "Epoch: 0163 D loss: -0.5815 G loss: -2.503\n",
      "Epoch: 0164 D loss: -0.5301 G loss: -2.612\n",
      "Epoch: 0165 D loss: -0.2869 G loss: -3.147\n",
      "Epoch: 0166 D loss: -0.5 G loss: -2.73\n",
      "Epoch: 0167 D loss: -0.4554 G loss: -2.682\n",
      "Epoch: 0168 D loss: -0.5457 G loss: -2.505\n",
      "Epoch: 0169 D loss: -0.8262 G loss: -1.606\n",
      "Epoch: 0170 D loss: -0.5097 G loss: -2.705\n",
      "Epoch: 0171 D loss: -0.4365 G loss: -2.777\n",
      "Epoch: 0172 D loss: -0.7252 G loss: -1.863\n",
      "Epoch: 0173 D loss: -0.4521 G loss: -2.684\n",
      "Epoch: 0174 D loss: -0.5769 G loss: -2.404\n",
      "Epoch: 0175 D loss: -0.5325 G loss: -2.514\n",
      "Epoch: 0176 D loss: -0.3797 G loss: -2.516\n",
      "Epoch: 0177 D loss: -0.3287 G loss: -2.811\n",
      "Epoch: 0178 D loss: -0.6006 G loss: -2.464\n",
      "Epoch: 0179 D loss: -0.4133 G loss: -2.381\n",
      "Epoch: 0180 D loss: -0.3921 G loss: -2.486\n",
      "Epoch: 0181 D loss: -0.3398 G loss: -2.5\n",
      "Epoch: 0182 D loss: -0.4194 G loss: -2.485\n",
      "Epoch: 0183 D loss: -0.3491 G loss: -2.63\n",
      "Epoch: 0184 D loss: -0.4501 G loss: -2.752\n",
      "Epoch: 0185 D loss: -0.3768 G loss: -2.511\n",
      "Epoch: 0186 D loss: -0.4107 G loss: -2.627\n",
      "Epoch: 0187 D loss: -0.5064 G loss: -2.406\n",
      "Epoch: 0188 D loss: -0.4218 G loss: -2.611\n",
      "Epoch: 0189 D loss: -0.5128 G loss: -2.515\n",
      "Epoch: 0190 D loss: -0.4897 G loss: -2.688\n",
      "Epoch: 0191 D loss: -0.4406 G loss: -2.829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0192 D loss: -0.2223 G loss: -3.155\n",
      "Epoch: 0193 D loss: -0.3971 G loss: -2.864\n",
      "Epoch: 0194 D loss: -0.5895 G loss: -2.127\n",
      "Epoch: 0195 D loss: -0.3555 G loss: -2.817\n",
      "Epoch: 0196 D loss: -0.4983 G loss: -2.529\n",
      "Epoch: 0197 D loss: -0.4845 G loss: -2.475\n",
      "Epoch: 0198 D loss: -0.4554 G loss: -2.311\n",
      "Epoch: 0199 D loss: -0.3856 G loss: -2.829\n",
      "Epoch: 0200 D loss: -0.5306 G loss: -2.915\n",
      "Epoch: 0201 D loss: -0.4682 G loss: -2.748\n",
      "Epoch: 0202 D loss: -0.5002 G loss: -2.806\n",
      "Epoch: 0203 D loss: -0.4382 G loss: -2.812\n",
      "Epoch: 0204 D loss: -0.4728 G loss: -3.009\n",
      "Epoch: 0205 D loss: -0.4251 G loss: -2.65\n",
      "Epoch: 0206 D loss: -0.3938 G loss: -2.59\n",
      "Epoch: 0207 D loss: -0.4403 G loss: -2.776\n",
      "Epoch: 0208 D loss: -0.4044 G loss: -2.836\n",
      "Epoch: 0209 D loss: -0.4667 G loss: -2.31\n",
      "Epoch: 0210 D loss: -0.4287 G loss: -2.598\n",
      "Epoch: 0211 D loss: -0.4767 G loss: -2.791\n",
      "Epoch: 0212 D loss: -0.521 G loss: -2.542\n",
      "Epoch: 0213 D loss: -0.468 G loss: -2.848\n",
      "Epoch: 0214 D loss: -0.3849 G loss: -2.647\n",
      "Epoch: 0215 D loss: -0.4496 G loss: -2.397\n",
      "Epoch: 0216 D loss: -0.3824 G loss: -2.886\n",
      "Epoch: 0217 D loss: -0.455 G loss: -2.65\n",
      "Epoch: 0218 D loss: -0.3775 G loss: -2.917\n",
      "Epoch: 0219 D loss: -0.3651 G loss: -2.785\n",
      "Epoch: 0220 D loss: -0.3949 G loss: -3.402\n",
      "Epoch: 0221 D loss: -0.4453 G loss: -2.599\n",
      "Epoch: 0222 D loss: -0.4481 G loss: -2.703\n",
      "Epoch: 0223 D loss: -0.4232 G loss: -2.771\n",
      "Epoch: 0224 D loss: -0.3975 G loss: -2.893\n",
      "Epoch: 0225 D loss: -0.3897 G loss: -2.798\n",
      "Epoch: 0226 D loss: -0.4373 G loss: -2.683\n",
      "Epoch: 0227 D loss: -0.338 G loss: -2.778\n",
      "Epoch: 0228 D loss: -0.3995 G loss: -2.809\n",
      "Epoch: 0229 D loss: -0.4369 G loss: -2.595\n",
      "Epoch: 0230 D loss: -0.4398 G loss: -2.859\n",
      "Epoch: 0231 D loss: -0.4995 G loss: -2.726\n",
      "Epoch: 0232 D loss: -0.4926 G loss: -2.654\n",
      "Epoch: 0233 D loss: -0.4254 G loss: -2.549\n",
      "Epoch: 0234 D loss: -0.4658 G loss: -3.152\n",
      "Epoch: 0235 D loss: -0.5641 G loss: -3.005\n",
      "Epoch: 0236 D loss: -0.3723 G loss: -2.922\n",
      "Epoch: 0237 D loss: -0.4375 G loss: -2.617\n",
      "Epoch: 0238 D loss: -0.3587 G loss: -2.832\n",
      "Epoch: 0239 D loss: -0.4243 G loss: -2.921\n",
      "Epoch: 0240 D loss: -0.5211 G loss: -2.639\n",
      "Epoch: 0241 D loss: -0.3739 G loss: -2.848\n",
      "Epoch: 0242 D loss: -0.3951 G loss: -3.114\n",
      "Epoch: 0243 D loss: -0.3282 G loss: -3.022\n",
      "Epoch: 0244 D loss: -0.3398 G loss: -2.958\n",
      "Epoch: 0245 D loss: -0.4215 G loss: -2.656\n",
      "Epoch: 0246 D loss: -0.4636 G loss: -2.79\n",
      "Epoch: 0247 D loss: -0.4872 G loss: -3.097\n",
      "Epoch: 0248 D loss: -0.4564 G loss: -2.595\n",
      "Epoch: 0249 D loss: -0.4617 G loss: -2.977\n",
      "Epoch: 0250 D loss: -0.3701 G loss: -2.707\n",
      "Epoch: 0251 D loss: -0.417 G loss: -2.792\n",
      "Epoch: 0252 D loss: -0.3524 G loss: -2.918\n",
      "Epoch: 0253 D loss: -0.4383 G loss: -3.058\n",
      "Epoch: 0254 D loss: -0.4282 G loss: -3.036\n",
      "Epoch: 0255 D loss: -0.3575 G loss: -2.801\n",
      "Epoch: 0256 D loss: -0.4288 G loss: -2.983\n",
      "Epoch: 0257 D loss: -0.4549 G loss: -2.922\n",
      "Epoch: 0258 D loss: -0.2905 G loss: -2.953\n",
      "Epoch: 0259 D loss: -0.4203 G loss: -2.894\n",
      "Epoch: 0260 D loss: -0.3649 G loss: -3.096\n",
      "Epoch: 0261 D loss: -0.3908 G loss: -3.109\n",
      "Epoch: 0262 D loss: -0.4416 G loss: -2.736\n",
      "Epoch: 0263 D loss: -0.4751 G loss: -3.22\n",
      "Epoch: 0264 D loss: -0.4137 G loss: -2.82\n",
      "Epoch: 0265 D loss: -0.3921 G loss: -3.387\n",
      "Epoch: 0266 D loss: -0.5076 G loss: -2.707\n",
      "Epoch: 0267 D loss: -0.4623 G loss: -2.869\n",
      "Epoch: 0268 D loss: -0.3927 G loss: -2.885\n",
      "Epoch: 0269 D loss: -0.4442 G loss: -2.986\n",
      "Epoch: 0270 D loss: -0.3268 G loss: -2.838\n",
      "Epoch: 0271 D loss: -0.4709 G loss: -2.483\n",
      "Epoch: 0272 D loss: -0.3894 G loss: -2.904\n",
      "Epoch: 0273 D loss: -0.528 G loss: -2.969\n",
      "Epoch: 0274 D loss: -0.4994 G loss: -2.686\n",
      "Epoch: 0275 D loss: -0.4054 G loss: -2.817\n",
      "Epoch: 0276 D loss: -0.3624 G loss: -3.057\n",
      "Epoch: 0277 D loss: -0.4502 G loss: -2.773\n",
      "Epoch: 0278 D loss: -0.4923 G loss: -3.319\n",
      "Epoch: 0279 D loss: -0.5452 G loss: -3.077\n",
      "Epoch: 0280 D loss: -0.4453 G loss: -2.698\n",
      "Epoch: 0281 D loss: -0.4902 G loss: -2.983\n",
      "Epoch: 0282 D loss: -0.4607 G loss: -2.676\n",
      "Epoch: 0283 D loss: -0.359 G loss: -2.84\n",
      "Epoch: 0284 D loss: -0.368 G loss: -3.361\n",
      "Epoch: 0285 D loss: -0.464 G loss: -3.165\n",
      "Epoch: 0286 D loss: -0.4454 G loss: -3.043\n",
      "Epoch: 0287 D loss: -0.2568 G loss: -3.324\n",
      "Epoch: 0288 D loss: -0.3586 G loss: -2.946\n",
      "Epoch: 0289 D loss: -0.5274 G loss: -2.776\n",
      "Epoch: 0290 D loss: -0.5091 G loss: -2.626\n",
      "Epoch: 0291 D loss: -0.3192 G loss: -3.052\n",
      "Epoch: 0292 D loss: -0.4167 G loss: -3.071\n",
      "Epoch: 0293 D loss: -0.3535 G loss: -3.467\n",
      "Epoch: 0294 D loss: -0.5021 G loss: -2.872\n",
      "Epoch: 0295 D loss: -0.5377 G loss: -2.817\n",
      "Epoch: 0296 D loss: -0.5169 G loss: -3.076\n",
      "Epoch: 0297 D loss: -0.4418 G loss: -2.516\n",
      "Epoch: 0298 D loss: -0.4012 G loss: -3.521\n",
      "Epoch: 0299 D loss: -0.3547 G loss: -3.024\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_batch=int(mnist.train.num_examples/ batch_size)\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        loss_val_D, loss_val_G = 0, 0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys= mnist.train.next_batch(batch_size)\n",
    "            noise=get_noise(batch_size, n_noise)\n",
    "            \n",
    "            _, loss_val_D = sess.run([train_D, loss_D], feed_dict={X: batch_xs, Z:noise})\n",
    "            _, loss_val_G = sess.run([train_G, loss_G], feed_dict={Z:noise})\n",
    "        \n",
    "        print('Epoch:', '%04d' % epoch, 'D loss: {:.4}'.format(loss_val_D), 'G loss: {:.4}'.format(loss_val_G))\n",
    "        \n",
    "        if epoch ==0 or (epoch+1) % 10 == 0:\n",
    "            sample_size=10\n",
    "            noise = get_noise(sample_size, n_noise)\n",
    "            samples=sess.run(G, feed_dict={Z:noise})\n",
    "            \n",
    "            fig,ax=plt.subplots(1, sample_size, figsize=(sample_size,1))\n",
    "            \n",
    "            for i in range(sample_size):\n",
    "                ax[i].set_axis_off()\n",
    "                ax[i].imshow(np.reshape(samples[i],(28,28)))\n",
    "                \n",
    "            plt.savefig('{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "            \n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
